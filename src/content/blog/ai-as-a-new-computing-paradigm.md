---
title: "AI as a New Computing Paradigm"
description: "Every computing shift follows the same pattern: a new primitive appears, an OS layer forms, applications emerge. AI is doing it again — and humans are moving up, not out."
pubDate: 2026-02-19
tags: ["ai", "llms", "computing", "opinion"]
draft: false
---

Every major computing shift follows the same pattern. A new compute primitive appears. An operating layer forms around it. Applications emerge on top. Humans outsource a category of work — and move up the ladder.

We've seen this before. We're seeing it again. And people are misreading it the same way they always do.

The key idea is simple:

> First we outsourced what we could specify. Now we're outsourcing what we can't specify — but can evaluate. Humans move up from doing to specifying to directing to judging to owning consequences.

This isn't replacement. It's repositioning.

## Two Paradigms, Same Shape

### The Old Stack: Deterministic Computing

The computing paradigm we've lived in for seventy years has three layers:

**CPU** — executes explicit instructions. Deterministic. Precise. Binary correctness. You tell it exactly what to do, and it does exactly that, billions of times per second.

**Operating System** — manages memory, processes, I/O. Schedules execution. Enforces permissions. Provides the primitives that applications build on.

**Applications** — user-facing programs built on OS primitives. Spreadsheets, databases, ERPs. They encode workflows humans can specify exactly.

This stack is extraordinary when requirements are precise, rules can be written explicitly, edge cases are enumerable, and success is binary. Correct or incorrect. No middle ground.

### The New Stack: Probabilistic Computing

Now look at what's forming:

**LLM (Probabilistic Compute Core)** — interprets intent. Reasons over ambiguity. Operates via probabilities, not instructions. Trained, not programmed. This isn't a faster CPU. It's a fundamentally different kind of compute primitive.

**Agentic Harness (OS Layer)** — memory management (short and long term), orchestration, tool routing, evaluation loops, permissions and identity, lifecycle management, observability. This is the operating system for probabilistic compute — and it's being built right now, in real time, by thousands of teams.

**Agents (Applications)** — goal-driven systems that maintain context, use tools, interact with humans, and persist over time. These are the "apps" of the new paradigm.

Same three-layer pattern. Completely different substrate.

## What We Already Outsourced

Before computers, humans did arithmetic. Managed inventories by hand. Performed bookkeeping. Coordinated logistics with paper and phones. Ran calculations manually.

Then deterministic computing arrived, and machines took over accounting, payroll, scheduling, data processing — everything that could be fully specified as a procedure.

Humans didn't disappear. They moved *up*. Into management, planning, design, creative work, decision making. The work that required judgment, not calculation.

Here's the pattern: **if something can be fully specified, it eventually gets automated.** That was the deal with deterministic computing. You write the rules, the machine follows them. Anything you can turn into a procedure, the computer will do faster, cheaper, and more reliably than you ever could.

But there's a ceiling.

## The Ceiling

Classical computers require explicit procedures, formal rules, and closed-world assumptions. They are fundamentally bad at ambiguity, natural language, context, judgment, aesthetics, and open-ended reasoning.

This isn't a bug. It's structural. Deterministic compute processes what can be specified. It can't process what can't. And a massive amount of real work — the work humans actually spend their days doing — lives in the fuzzy, unspecifiable space.

Drafting an email. Summarizing a meeting. Researching a market. Exploring an idea. Synthesizing conflicting information. Making a judgment call with incomplete data.

None of this can be reduced to explicit procedures. That's why it stayed with humans for seventy years while everything else got automated.

## What Changes with AI

LLMs don't require full specifications. They work from examples, patterns, intent, and probability. This is what makes them a genuinely new compute primitive — not a better version of the old one.

This unlocks outsourcing of an entirely new category: drafting, summarizing, researching, exploring ideas, synthesizing information, fuzzy reasoning. Tasks that were never precisely specifiable.

But here's the critical insight — they *are* evaluatable. You can't write a spec for "draft a good email." But you can read a draft and know whether it's good. You can't write rules for "summarize this research." But you can read a summary and judge whether it captures the important parts.

**We're not outsourcing judgment. We're outsourcing generation.**

Humans still evaluate outputs. Set direction. Choose goals. Define constraints. Own failures. The role shifts from doing to directing to judging. You stop being the person who drafts and become the person who evaluates drafts. You stop being the person who researches and become the person who judges whether the research is thorough enough.

## Why This Feels Messy

We are extremely early. And early always looks broken.

The agent OS layer is incomplete. Memory is primitive. Orchestration is fragile. Evaluation is mostly manual. We're building the operating system for a new compute paradigm while simultaneously trying to build applications on top of it. Of course it's messy.

"Agents aren't reliable" — correct. The OS layer is still forming. Early operating systems crashed constantly too. That doesn't mean the paradigm is wrong. It means the infrastructure is young.

"Fuzzy tasks are high-stakes" — also correct. Which is why adoption starts with drafts, copilots, and exploration. Not autonomous decision-making. Authority comes later, once the OS layer matures, evaluation gets better, and trust is earned through track record. The same way we didn't put critical financial systems on computers in 1955.

## This Is Layering, Not Replacement

Here's what people keep getting wrong: they think AI replaces deterministic computing. It doesn't. It layers on top.

Deterministic systems remain the execution substrate. When an LLM decides something needs to happen, a deterministic system executes it. The API call is still an API call. The database write is still a database write. The deployment pipeline is still a deployment pipeline.

AI becomes a cognitive overlay. The LLM reasons and decides. Deterministic systems execute precisely. Humans supervise and own outcomes. Three layers of a complete system, each doing what it's best at.

## Where Humans Move Next

Every automation wave makes the same set of skills scarce: the ones the machines can't do. With deterministic computing, that was creativity and judgment. With probabilistic computing, the scarce skills shift again.

What becomes valuable: taste and coherence. Goal setting. Value judgments. Accountability. Governance. Relationship building. System design. The ability to look at an AI-generated output and know — not calculate, *know* — whether it's right.

Humans become directors, evaluators, stewards, responsibility holders. Not because those are consolation prizes. Because those are the things that actually matter, and they've always been bottlenecked by the fact that the humans doing them were also stuck doing the generation work underneath.

When you free someone from drafting, they have more bandwidth for judgment. When you free someone from research, they have more bandwidth for synthesis. The human doesn't do less. The human does *higher*.

## The Product Engineer as a Natural Outcome

This is why the [Product Engineer](/posts/the-rise-of-the-product-engineer) role makes sense as more than a trend. It's the natural outcome of this paradigm shift.

A Product Engineer directs agents. Iterates through live systems. Judges quality. Holds product coherence. Owns outcomes. Engineering becomes orchestration and judgment — which is exactly what you'd expect when the generation layer gets outsourced to a new compute primitive.

## The Real Shift

Each computing revolution removes a layer of work beneath humans. The deterministic revolution removed clerical labor. The probabilistic revolution is removing fuzzy execution — the drafting, the researching, the synthesizing, the generating.

What remains is judgment, taste, and responsibility. The things that are uniquely, irreducibly human. Not because machines can't be trained to mimic them, but because someone still has to own the consequences. Someone still has to decide what *should* exist, not just what *can*.

We're not watching humans get replaced. We're watching humans climb.

From execution to direction. From production to judgment. From procedures to purpose.

That's the real shift of the AI era.
